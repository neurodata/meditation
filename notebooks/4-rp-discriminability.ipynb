{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.discriminability import discr_stat\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import euclidean_distances\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_key = 'latent'\n",
    "## Define paths\n",
    "basedir = Path('..')\n",
    "datadir = basedir / 'data'\n",
    "rawdir = datadir / 'raw'\n",
    "gccadir = datadir / 'interim' / 'gcca250'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(path,\n",
    "              level='(e|n)',\n",
    "              subject='([0-9]{3})',\n",
    "              task='(.+?)',\n",
    "              filetype='h5',\n",
    "              flag=''):\n",
    "    files = []\n",
    "    query = f'^{level}_sub-'\n",
    "    query += f'{subject}_ses-1_'\n",
    "    query += f'task-{task}{flag}\\.{filetype}'\n",
    "    for f in os.listdir(path):\n",
    "        match = re.search(query, f)\n",
    "        if match:\n",
    "            files.append((f, match.groups()))\n",
    "    \n",
    "    return(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = ['restingstate', 'openmonitoring', 'compassion']\n",
    "levels = ['e', 'n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:00<00:00, 62.13it/s]\n",
      "100%|██████████| 29/29 [00:00<00:00, 67.61it/s]\n",
      "100%|██████████| 29/29 [00:00<00:00, 58.89it/s]\n",
      "100%|██████████| 47/47 [00:00<00:00, 78.35it/s]\n",
      "100%|██████████| 47/47 [00:00<00:00, 80.09it/s]\n",
      "100%|██████████| 47/47 [00:00<00:00, 89.03it/s]\n"
     ]
    }
   ],
   "source": [
    "## Get filenames for each task, novice vs. experienced\n",
    "## Load a single set of latents\n",
    "\n",
    "#latents_inter = {l:{t:[] for t in tasks} for l in levels}\n",
    "#labels_inter = {l:{t:[] for t in tasks} for l in levels}\n",
    "\n",
    "#latents_intra = {t:{l:[] for l in levels} for t in tasks}\n",
    "#labels_intra = {t:{l:[] for l in levels} for t in tasks}\n",
    "\n",
    "latents = []; labels_lt = []; labels_l = []; labels_t = []\n",
    "\n",
    "n_components = 1\n",
    "\n",
    "for level in levels:\n",
    "    for task in tasks:\n",
    "        paths = get_files(path=gccadir, level=level, task=task, flag='_gcca')\n",
    "        \n",
    "        n_load = len(paths)\n",
    "\n",
    "        for path,subj in tqdm(paths[:n_load]):\n",
    "            h5f = h5py.File(gccadir / path,'r')\n",
    "            latent = h5f[h5_key][:][:,0]\n",
    "            h5f.close()\n",
    "            \n",
    "            latents.append(latent)\n",
    "            labels_lt.append(f'{level}_{task}')\n",
    "            labels_l.append(level)\n",
    "            labels_t.append(task)\n",
    "            \n",
    "            \n",
    "\n",
    "labels_lt = np.array(labels_lt)\n",
    "labels_t = np.array(labels_t)\n",
    "labels_l = np.array(labels_l)\n",
    "latents = np.array(latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "discr_index, rdfs = discr_stat(latents,labels_l, return_rdfs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rflperry/miniconda3/envs/datasci/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_restingstate, n_restingstate: Discr Index=0.5086847074252225\n",
      "e_openmonitoring, n_openmonitoring: Discr Index=0.5280508846293704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rflperry/miniconda3/envs/datasci/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/rflperry/miniconda3/envs/datasci/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_compassion, n_compassion: Discr Index=0.4805657345317525\n"
     ]
    }
   ],
   "source": [
    "for task in tasks:\n",
    "    ls = [f'{level}_{task}' for level in levels]\n",
    "    idx = np.hstack((np.where(np.isin(labels_lt, l)) for l in ls))[0]\n",
    "    discr_index = discr_stat(latents[idx],labels_lt[idx])\n",
    "    print(f'{ls[0]}, {ls[1]}: Discr Index={discr_index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rflperry/miniconda3/envs/datasci/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_restingstate, e_openmonitoring: Discr Index=0.49485448162618195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rflperry/miniconda3/envs/datasci/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_restingstate, n_openmonitoring: Discr Index=0.49765780305863366\n"
     ]
    }
   ],
   "source": [
    "for level in levels:\n",
    "    ls = [f'{level}_{task}' for task in tasks]\n",
    "    idx = np.hstack((np.where(np.isin(labels_lt, l)) for l in ls))[0]\n",
    "    discr_index = discr_stat(latents[idx],labels_lt[idx])\n",
    "    print(f'{ls[0]}, {ls[1]}: Discr Index={discr_index}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Distance Matrices for R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "distancedir = datadir / 'interim' / 'gcca250_distances'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_save_classes(labels, level1='',task1='',level2='',task2='',task3=''):\n",
    "    global latents\n",
    "    ## Create search keys and get indices\n",
    "    key1 = f'{level1}_{task1}'\n",
    "    key2 = f'{level2}_{task2}'\n",
    "    idx1 = [i for i,label in enumerate(labels) if key1 in label]\n",
    "    idx2 = [i for i,label in enumerate(labels) if key2 in label]\n",
    "    print(f'Len of {key1}: {len(idx1)}')\n",
    "    print(f'Len of {key2}: {len(idx2)}')\n",
    "    if not task3 == '':\n",
    "        key3 = f'_{task3}'\n",
    "        idx3 = [i for i,label in enumerate(labels) if key3 in label]\n",
    "        print(f'Len of {key3}: {len(idx3)}')\n",
    "        idxs = np.hstack((idx1, idx2, idx3))\n",
    "        \n",
    "        ## Get relevant stuff\n",
    "        distances = euclidean_distances(latents[idxs])\n",
    "        labels2 = np.hstack((['1'] * len(idx1), ['2'] * len(idx2), ['3'] * len(idx3)))\n",
    "\n",
    "        ## Save relevant stuff\n",
    "        pd.DataFrame(distances).to_csv(distancedir / f'{key1}_{key2}_{key3}_distances.csv', header=False, index=False)\n",
    "        pd.DataFrame(labels2).to_csv(distancedir / f'{key1}_{key2}_{key3}_labels.csv', header=False, index=False)\n",
    "    else:\n",
    "        idxs = np.hstack((idx1, idx2))\n",
    "    \n",
    "        ## Get relevant stuff\n",
    "        distances = euclidean_distances(latents[idxs])\n",
    "        labels2 = np.hstack((['1'] * len(idx1), ['2'] * len(idx2)))\n",
    "\n",
    "        ## Save relevant stuff\n",
    "        pd.DataFrame(distances).to_csv(distancedir / f'{key1}_{key2}_distances.csv', header=False, index=False)\n",
    "        pd.DataFrame(labels2).to_csv(distancedir / f'{key1}_{key2}_labels.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of e_restingstate: 29\n",
      "Len of n_restingstate: 47\n",
      "Len of e_openmonitoring: 29\n",
      "Len of n_openmonitoring: 47\n",
      "Len of e_compassion: 29\n",
      "Len of n_compassion: 47\n"
     ]
    }
   ],
   "source": [
    "## Inter task (3)\n",
    "for task in tasks:\n",
    "    get_save_classes(labels=labels_lt, level1=levels[0], level2=levels[1], task1=task, task2=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of e_restingstate: 29\n",
      "Len of e_openmonitoring: 29\n",
      "Len of e_restingstate: 29\n",
      "Len of e_compassion: 29\n",
      "Len of e_openmonitoring: 29\n",
      "Len of e_compassion: 29\n",
      "Len of n_restingstate: 47\n",
      "Len of n_openmonitoring: 47\n",
      "Len of n_restingstate: 47\n",
      "Len of n_compassion: 47\n",
      "Len of n_openmonitoring: 47\n",
      "Len of n_compassion: 47\n"
     ]
    }
   ],
   "source": [
    "## Inter experience (2)\n",
    "for level in levels:\n",
    "    for t1,t2 in combinations(tasks, 2):\n",
    "        get_save_classes(labels=labels_lt, level1=level, level2=level, task1=t1, task2=t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of e_restingstate: 29\n",
      "Len of n_openmonitoring: 47\n",
      "Len of e_openmonitoring: 29\n",
      "Len of n_restingstate: 47\n",
      "Len of e_restingstate: 29\n",
      "Len of n_compassion: 47\n",
      "Len of e_compassion: 29\n",
      "Len of n_restingstate: 47\n",
      "Len of e_openmonitoring: 29\n",
      "Len of n_compassion: 47\n",
      "Len of e_compassion: 29\n",
      "Len of n_openmonitoring: 47\n"
     ]
    }
   ],
   "source": [
    "## Pairwise (9)\n",
    "## Inter experience (2)\n",
    "for t1,t2 in combinations(tasks, 2):\n",
    "    get_save_classes(labels=labels_lt, level1=levels[0], level2=levels[1], task1=t1, task2=t2)\n",
    "    get_save_classes(labels=labels_lt, level1=levels[0], level2=levels[1], task1=t2, task2=t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of e_: 87\n",
      "Len of n_: 141\n"
     ]
    }
   ],
   "source": [
    "## Novice vs. Expert (9)\n",
    "get_save_classes(labels=labels_lt, level1=levels[0], level2=levels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of _restingstate: 76\n",
      "Len of _openmonitoring: 76\n",
      "Len of _restingstate: 76\n",
      "Len of _compassion: 76\n",
      "Len of _openmonitoring: 76\n",
      "Len of _compassion: 76\n"
     ]
    }
   ],
   "source": [
    "## Inter-trait (3)\n",
    "for t1,t2 in itertools.combinations(tasks, 2):\n",
    "    get_save_classes(labels=labels_lt, task1=t1, task2=t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of _restingstate: 76\n",
      "Len of _openmonitoring: 76\n",
      "Len of _compassion: 76\n"
     ]
    }
   ],
   "source": [
    "## Triplet inter-trait (1)\n",
    "get_save_classes(labels=labels_lt, task1=tasks[0], task2=tasks[1], task3=tasks[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:datasci] *",
   "language": "python",
   "name": "conda-env-datasci-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
