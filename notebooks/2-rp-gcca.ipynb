{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from gcca import GCCA\n",
    "import logging\n",
    "# set log level\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "import numpy as np\n",
    "from scipy import linalg,stats\n",
    "from scipy.sparse.linalg import svds\n",
    "# from sklearn.decomposition import TruncatedSVD\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "## See https://github.com/rupy/GCCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = Path('..')\n",
    "datadir = basedir / 'data' / 'raw'\n",
    "correlation_dir = basedir / 'data' / 'interim' / 'latents'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grab filenames\n",
    "def get_files(level='(e|n)',\n",
    "              subject='[0-9]{3}',\n",
    "              task='.+?'):\n",
    "    files = []\n",
    "    query = f'^{level}_sub-'\n",
    "    query += f'({subject})_ses-1_'\n",
    "    query += f'task-{task}\\.csv'\n",
    "    for f in os.listdir(datadir):\n",
    "        match = re.search(query, f)\n",
    "        if match:\n",
    "            files.append((f, match.group(1)))\n",
    "    \n",
    "    return(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = ['restingstate', 'openmonitoring', 'compassion']\n",
    "levels = ['e', 'n']\n",
    "\n",
    "#for task,level in np.array(np.meshgrid(tasks,levels)).T.reshape(-1,2):\n",
    "#    paths = get_files(level=level, task=task)\n",
    "\n",
    "paths = get_files(level=levels[0], task=tasks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "subjs = []\n",
    "for path,subj in paths[:2]:\n",
    "    data.append(pd.read_csv(datadir / path, header = None).to_numpy())\n",
    "    subjs.append(subj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    x2 = stats.zscore(x,axis=1)\n",
    "    x2 -= np.mean(x2,axis=0)\n",
    "    return x2\n",
    "\n",
    "def gcca(data, rank_tolerance=None, n_components=None):\n",
    "    n = data[0].shape[0]\n",
    "    \n",
    "    Uall = []\n",
    "    Sall = []\n",
    "    Vall = []\n",
    "    ranks = []\n",
    "    for x in tqdm(data):\n",
    "        # Preprocess\n",
    "        x = preprocess(x)\n",
    "        x[np.isnan(x)] = 0\n",
    "        \n",
    "        # compute the SVD of the data\n",
    "        v,s,ut = linalg.svd(x.T, full_matrices=False)\n",
    "        \n",
    "        Sall.append(s)\n",
    "        Vall.append(v.T)\n",
    "        # Dimensions to reduce to\n",
    "        if rank_tolerance:\n",
    "            rank = sum(S > rank_tolerance)\n",
    "        else:\n",
    "            rank = n_components\n",
    "        ranks.append(rank)\n",
    "        ut = ut.T[:,:rank]\n",
    "        Uall.append(ut)\n",
    "\n",
    "    d = min(ranks)\n",
    "    \n",
    "    # Create a concatenated view of Us\n",
    "    Uall_c = np.concatenate(Uall,axis=1)\n",
    "\n",
    "    _,_,VV=svds(Uall_c,d)\n",
    "    VV = VV.T\n",
    "    VV = VV[:,:min([d,VV.shape[1]])]\n",
    "    \n",
    "    # SVDS the concatenated Us\n",
    "    idx_end = 0\n",
    "    projX = []\n",
    "    for i in range(len(data)):\n",
    "        idx_start = idx_end\n",
    "        idx_end = idx_start + ranks[i]\n",
    "        VVi = normalize(VV[idx_start:idx_end,:],'l2')\n",
    "        # Compute the canonical projections\n",
    "        A = np.sqrt(n-1) * Vall[i][:,:rank]\n",
    "        A = A @ (linalg.solve(np.diag(Sall[i][:rank]), VVi))\n",
    "        projX.append(data[i] @ A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_views = 2 # Number of subjects (views) to consider\n",
    "rank_tolerance = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:datasci] *",
   "language": "python",
   "name": "conda-env-datasci-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
